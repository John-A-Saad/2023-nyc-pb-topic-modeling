{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling on New York City Participatory Budget Proposals\n",
    "\n",
    "### Overview\n",
    "This analysis started by scraping approximately two thousand participatory budgeting proposals from New York City's [idea collection platform](https://www.participate.nyc.gov/processes/Citywidepb2023/f/321/) submitted during the last quarter of 2023, aiming to perform topic modeling to identify key ideas for potentially funded programs and services. A total of 1,947 proposals were collected, with 1,897 being analyzable; those excluded contained insufficient information. From these 1,897 proposals, 20 topics were generated and assigned back to each of the proposals. \n",
    "\n",
    "This topic modeling approach utilized [BERTopic](https://arxiv.org/abs/2203.05794), leveraging [OpenAI's embeddings](https://platform.openai.com/docs/guides/embeddings/) to transform each proposal into a high-dimensional vector, subsequently reduced via [UMAP](https://arxiv.org/abs/1802.03426) and clustered using [HBDSCAN](https://arxiv.org/abs/1911.02282). Each cluster was considered a \"big document\", where important keywords and n-grams (ranging from 1 to 3 words) were extracted using to define topics. [OpenAI's GPT-4o mini](https://platform.openai.com/docs/models/gpt-4o-mini) further refined these topics into coherent descriptions based on the clusters' prominent proposals and keywords.  This analysis illustrates a scalable approach to understanding civic engagement through data-driven insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages, Constants, and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import re\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import plotly.express as px\n",
    "\n",
    "# NLP\n",
    "import openai \n",
    "from bertopic.backend import OpenAIBackend \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from bertopic.representation import OpenAI, KeyBERTInspired, MaximalMarginalRelevance\n",
    "from bertopic import BERTopic \n",
    "import spacy\n",
    "\n",
    "# Dimensionality reduction\n",
    "from sklearn.decomposition import PCA \n",
    "from umap import UMAP \n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# Tokenization\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.corpora import Dictionary\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Weighting\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "\n",
    "# Evaluation \n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Shhh\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set the seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Get large Spacy model for lemmatization\n",
    "nlp = spacy.load(\"en_core_web_lg\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "# URL pattern for removal\n",
    "url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "\n",
    "# Experimental seed words for steering the topic model (not used in the final model)\n",
    "seed_words = [\n",
    "    \"art\", \"gallery\", \"theater\", \"exhibition\", \"festival\", \"cultural\", \"heritage\", \"performance\", \"music\", \"dance\",\n",
    "    \"community\", \"assistance\", \"services\", \"financial\", \"digital\", \"esl\", \"english\", \"ged\", \"support\", \"resources\",\n",
    "    \"jobs\", \"training\", \"skills\", \"career\", \"entrepreneurship\", \"business\", \"workforce\", \"economic\", \"empowerment\", \"workshops\",\n",
    "    \"education\", \"youth\", \"families\", \"school\", \"learning\", \"children\", \"seniors\", \"adult\", \"parenting\", \"engagement\",\n",
    "    \"environment\", \"sustainability\", \"eco-friendly\", \"green\", \"food\", \"organic\", \"recycling\", \"conservation\", \"justice\", \"life\",\n",
    "    \"parks\", \"recreation\", \"facilities\", \"space\", \"playgrounds\", \"sports\", \"outdoor\", \"garden\", \"safety\", \"accessibility\",\n",
    "    \"health\", \"wellness\", \"mental\", \"physical\", \"fitness\", \"well-being\", \"healthcare\", \"prevention\", \"living\", \"therapy\",\n",
    "    \"safety\", \"justice\", \"prevention\", \"policing\", \"reduction\", \"recovery\", \"abuse\", \"secure\", \"services\", \"emergency\",\n",
    "    \"streets\", \"sidewalks\", \"infrastructure\", \"transportation\", \"accessibility\", \"pedestrian\", \"safety\", \"mobility\", \"biking\", \"planning\",\n",
    "    \"miscellaneous\", \"various\", \"diverse\", \"general\", \"flexible\", \"undefined\", \"exploratory\", \"innovative\", \"creative\", \"unspecified\"\n",
    "]\n",
    "\n",
    "\n",
    "def get_embedding(text: str, client: openai.OpenAI, model: str) -> np.ndarray:\n",
    "   '''Get the embeddings for the text using the specified model\n",
    "   \n",
    "   Args:\n",
    "   - text: str, the text to get the embeddings for\n",
    "   - client: openai.OpenAI, the OpenAI client to use\n",
    "   - model: str, the model to use\n",
    "   \n",
    "   Returns:\n",
    "   - np.ndarray, the embeddings for the text\n",
    "   '''\n",
    "   return client.embeddings.create(\n",
    "      input=[text], \n",
    "      model=model\n",
    "    ).data[0].embedding\n",
    "\n",
    "def get_coherence_score(topic_words: list, dictionary: Dictionary, tokenized_docs: list) -> float:\n",
    "    '''Calculate the Cv coherence score for the topics\n",
    "    \n",
    "    Args:\n",
    "    - topic_words: list of lists of strings, the topics to calculate the coherence score for\n",
    "    - dictionary: Dictionary, the dictionary created from the tokenized documents\n",
    "    - tokenized_docs: list of lists of strings, the tokenized documents\n",
    "    \n",
    "    Returns:\n",
    "    - coherence_score: float, the Cv coherence score for the topics\n",
    "    '''\n",
    "    coherence_model = CoherenceModel(\n",
    "        topics=topic_words, \n",
    "        texts=tokenized_docs, \n",
    "        dictionary=dictionary, \n",
    "        coherence='c_v'\n",
    "    )\n",
    "    return coherence_model.get_coherence()\n",
    "\n",
    "def get_diversity(topics: list , top_n=10) -> float:\n",
    "    '''Calculate the diversity score for the topics\n",
    "    \n",
    "    Args:\n",
    "    - topics: list of lists of strings, the topics to calculate the diversity score for\n",
    "    - top_n: int, the number of top words to consider for each topic\n",
    "    \n",
    "    Returns:\n",
    "    - diversity_score: float, the diversity score for the topics\n",
    "    '''\n",
    "    unique_words = set()\n",
    "    total_words = 0\n",
    "    for topic in topics:\n",
    "        unique_words.update(topic[:top_n])\n",
    "        total_words += top_n\n",
    "    diversity_score = len(unique_words) / total_words\n",
    "    return diversity_score\n",
    "\n",
    "def tokenize_docs_and_create_dictionary(docs: list) -> tuple[list, Dictionary]:\n",
    "    '''Tokenize the documents and create a dictionary\n",
    "    \n",
    "    Args:\n",
    "    - docs: list of strings, the documents to tokenize\n",
    "    \n",
    "    Returns:\n",
    "    - tokenized_docs: list of lists of strings, the tokenized documents\n",
    "    - dictionary: Dictionary, the dictionary created from the tokenized documents\n",
    "    '''\n",
    "    # Download the punkt tokenizer\n",
    "    nltk.download('punkt')  \n",
    "\n",
    "    # Tokenize each document\n",
    "    tokenized_docs = [word_tokenize(doc.lower()) for doc in docs]\n",
    "\n",
    "    # Create a dictionary from the tokenized documents\n",
    "    dictionary = Dictionary(tokenized_docs)\n",
    "\n",
    "    return tokenized_docs, dictionary\n",
    "\n",
    "def get_stopwords() -> list:\n",
    "    '''Get list of stop words to remove from the documents\n",
    "    \n",
    "    Returns:\n",
    "    - stop_words: list of strings, the stop words to remove\n",
    "    '''\n",
    "    # Define base stop words in the English language to remove\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "\n",
    "    # Add others custom stop words \n",
    "    # Any others words that are not useful for the analysis?\n",
    "    stop_words.update([\n",
    "        'said', 'participatory', 'budget', 'well', 'make', \n",
    "        'plan', '2025', 'see', 'best', 'think', 'could', \n",
    "        'came', 'consider', 'please', 'take', 'many', \n",
    "        'taken', 'would', 'go', 'coming', 'might', 'good', \n",
    "        'say', 'get', 'says', 'want', 'asking', 'thinking', \n",
    "        'know', 'people', 'thanks', 'ask', 'planning', \n",
    "        'thank', 'much', 'thoughts', 'great', 'made', \n",
    "        'come', 'thought', 'going', 'really', 'pb', 'also', \n",
    "        'may', 'need', 'should', 'taking', 'saying', 'way', \n",
    "        'considering', '25', 'goes', 'lot', 'community', \n",
    "        'city', 'better', 'public', 'asked', 'like', \n",
    "        'nyc', 'fy', 'considered', 'plans', 'one',\n",
    "        'year', 'years', 'next', 'last', 'first','second', \n",
    "        'third', 'fourth', 'fifth','choice', 'choices', \n",
    "        'option', 'options','something','anything', \n",
    "        'nothing', 'everything', 'september', 'october', \n",
    "        'november', 'december', 'january', 'february',\n",
    "        'march', 'april', 'may', 'june', 'july', 'august',\n",
    "        'upgrade', 'provide', 'ballot', 'project', 'projects',\n",
    "        'proposal', 'proposals', 'idea', 'ideas', 'initiative',\n",
    "        'initiatives', 'vote', 'voting', 'voted', 'votes',\n",
    "        'new', 'york','title:', 'tags:', 'challenge:', 'soultion:',\n",
    "        'title', 'tags', 'challenge', 'soultion','solution','tag',\n",
    "        'empower', 'empowerment', 'empowered', 'empowerment','vibrant',\n",
    "        'bronx', 'brooklyn', 'manhattan', 'queens', 'staten', 'island',\n",
    "        'dimitri', 'farah', 'maria','try', 'anh', 'ada', 'jean','nice',\n",
    "        'daughter', 'en','mas','haruto','zuri'])\n",
    "\n",
    "    return list(stop_words)\n",
    "\n",
    "def preprocess_document(doc) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess the document by cleaning and lemmatizing text.\n",
    "    \n",
    "    Args:\n",
    "    - doc: str, the document to preprocess.\n",
    "    \n",
    "    Returns:\n",
    "    - preprocessed_doc: str, the preprocessed document.\n",
    "    \"\"\"\n",
    "    # Lowercase and remove URLs\n",
    "    doc = doc.lower()\n",
    "    doc = url_pattern.sub('', doc)\n",
    "\n",
    "    # Process the document with spaCy\n",
    "    doc = nlp(doc)\n",
    "\n",
    "    # Lemmatize and remove punctuation, numbers, and stop words\n",
    "    lemmatized_text = []\n",
    "    for token in doc:\n",
    "        if token.is_alpha and not token.is_stop:\n",
    "            lemmatized_text.append(token.lemma_)\n",
    "    \n",
    "    # Join processed tokens back into a single string\n",
    "    preprocessed_doc= \" \".join(lemmatized_text)\n",
    "\n",
    "    return preprocessed_doc\n",
    "\n",
    "def extract_topic_model_words(topic_model: BERTopic) -> list:\n",
    "    '''Extract the topic words from the BERTopic model\n",
    "    \n",
    "    Args:\n",
    "    - topic_model: BERTopic, the trained BERTopic model\n",
    "    \n",
    "    Returns:\n",
    "    - bertopic_topic_words: list of lists of strings, the topic words from the model\n",
    "    '''\n",
    "    # Get the topics from the BERTopic model and filter out empty topics\n",
    "    bertopic_topics = [\n",
    "        topic_words \n",
    "        for topic_id, topic_words in topic_model.get_topics().items() \n",
    "        if (len(topic_words) > 0) and (topic_id != -1)\n",
    "    ]\n",
    "\n",
    "    # Make topics list of lists of strings instead of tuples with probabilities\n",
    "    bertopic_topic_words = [\n",
    "        [w[0] for w in t] \n",
    "        for t in bertopic_topics\n",
    "    ]\n",
    "\n",
    "    return bertopic_topic_words\n",
    "\n",
    "def evaluate_topic_model(topic_model: BERTopic, raw_documents: list, topics: list) -> dict:\n",
    "    '''Evaluate the BERTopic model using coherence and diversity scores\n",
    "    \n",
    "    Args:\n",
    "    - topic_model: BERTopic, the trained BERTopic model\n",
    "    - raw_documents: list of strings, the raw documents used to train the model\n",
    "    - topics: list of integers, the topic id assigned to each document\n",
    "    \n",
    "    Returns:\n",
    "    - evaluation: dict, the evaluation metrics for the model\n",
    "    '''\n",
    "    # Preprocess documents (Exclude -1 outlier topic from evaluation)\n",
    "    preprocessed_documents = [\n",
    "        preprocess_document(doc) \n",
    "        for doc, topic in zip(raw_documents, topics) \n",
    "        if topic != -1\n",
    "    ]\n",
    "\n",
    "    # Extract the topic words from the BERTopic model\n",
    "    bertopic_topic_words = extract_topic_model_words(topic_model)\n",
    "\n",
    "    # Tokenize the preprocessed documents and create a dictionary\n",
    "    tokenized_docs, dictionary = tokenize_docs_and_create_dictionary(preprocessed_documents)\n",
    "\n",
    "    # Calculate the coherence score\n",
    "    coherence_score = get_coherence_score(bertopic_topic_words, dictionary, tokenized_docs)\n",
    "\n",
    "    # Calculate the diversity score\n",
    "    diversity_score = get_diversity(bertopic_topic_words)\n",
    "\n",
    "    # print(f\"Coherence score: {coherence_score}\")\n",
    "    # print(f\"Diversity score: {diversity_score}\")\n",
    "    return {\n",
    "        \"coherence_score\": coherence_score,\n",
    "        \"diversity_score\": diversity_score\n",
    "    }\n",
    "\n",
    "def openai_representation(\n",
    "        openai_api_key: str,\n",
    "        model: str='gpt-4-turbo',\n",
    "        num_docs: int=5,\n",
    "        diversity: float=0.2\n",
    "    ) -> OpenAI:\n",
    "    '''Get the OpenAI representation model for the topics\n",
    "    \n",
    "    Args:\n",
    "    - openai_api_key: str, OpenAI API key\n",
    "    - model: str, the OpenAI model to use for the representation (default: 'gpt-4-turbo')\n",
    "    - num_docs: int, the number of documents to use for the representation (default: 5)\n",
    "    - diversity: float, the diversity of the topics (default: 0.2)\n",
    "\n",
    "    Returns:\n",
    "    - representation_model: OpenAI, the OpenAI representation model\n",
    "\n",
    "    Notes:\n",
    "        -  The cost is directly proportional to the\n",
    "            - type of model\n",
    "            - number of tokens in documents\n",
    "            - number of documents\n",
    "            - number of topics\n",
    "    '''\n",
    "    prompt = \"\"\"\n",
    "    Consider the following set of documents:\n",
    "    [DOCUMENTS]\n",
    "    These documents are associated with the keywords: [KEYWORDS]\n",
    "\n",
    "    Analyze the documents and keywords provided to generate a concise, descriptive label for the topic. The label should encapsulate the primary theme of the keywords and documents in no more than 10 words and the label should represent program ideas. Please provide the topic label in the format specified below:\n",
    "    topic: <topic label>\n",
    "    \"\"\"\n",
    "\n",
    "    # prompt = \"\"\"\n",
    "    # I have a topic that contains the following documents:\n",
    "    # [DOCUMENTS]\n",
    "    # The topic is described by the following keywords: [KEYWORDS]\n",
    "\n",
    "    # Based on the information above, extract a short but highly descriptive topic label of at most 5 words. Make sure it is in the following format:\n",
    "    # topic: <topic label>\n",
    "    # \"\"\"\n",
    "    client = openai.OpenAI(api_key=openai_api_key)\n",
    "    openai_model = OpenAI(\n",
    "        client, \n",
    "        model=model, \n",
    "        exponential_backoff=True, \n",
    "        chat=True, \n",
    "        prompt=prompt,\n",
    "        nr_docs=num_docs,\n",
    "        diversity=diversity\n",
    "    )\n",
    "\n",
    "    return openai_model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_excel('../data/nyc_pb_proposals.xlsx')\n",
    "\n",
    "# Reformate the datetime column\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# Remove rows with missing values in what will be the proposal column\n",
    "df = df[~df[['title','tags','Describe the challenge you want to address:','Describe your idea and approach to address the challenge']].isna().any(axis=1)]\n",
    "\n",
    "# Reset the index\n",
    "df.reset_index(drop=True, inplace=True) \n",
    "\n",
    "# Clean the proposal and challenge columns\n",
    "df['proposal'] = (df['Describe your idea and approach to address the challenge'].str.replace(r'(\\n){2,}|\"', '\\n', regex=True).str.strip()).str.strip()\n",
    "df['challenge'] = (df['Describe the challenge you want to address:'].str.replace(r'(\\n){2,}|\"', '\\n', regex=True).str.strip()).str.strip()\n",
    "\n",
    "# Combine the title, tags, challenge, and solution into a single column for the proposal\n",
    "# df['proposal'] = ('Title: ' + df['title'] \n",
    "#                   + '\\n\\nTags: ' + df['tags'].str.replace(\"\\[|\\]|'\",'',regex=True)\n",
    "#                   + '\\n\\nChallenge: ' + df['Describe the challenge you want to address:'].str.replace(r'(\\n){2,}|\"', '\\n', regex=True).str.strip()\n",
    "#                   + '\\n\\nSoultion: ' + df['Describe your idea and approach to address the challenge'].str.replace(r'(\\n){2,}|\"', '\\n', regex=True).str.strip()).str.strip()\n",
    "\n",
    "# Remove urls from the proposal and challenge columns\n",
    "df['proposal'] = df['proposal'].apply(lambda x: url_pattern.sub('', x)).str.strip()\n",
    "df['challenge'] = df['challenge'].apply(lambda x: url_pattern.sub('', x)).str.strip()\n",
    "\n",
    "# Rename the columns\n",
    "df.rename(\n",
    "    columns={\n",
    "        'In which borough should your idea take place?':'borough',\n",
    "        'Do you have a specific neighborhood(s) in mind?':'neighborhood',\n",
    "        'Which audience(s) does your idea help? Select as many as apply.':'audience',\n",
    "        'Write the zipcode that best represents your New York City community':'zipcode'\n",
    "    }, inplace=True\n",
    ")\n",
    "\n",
    "# Drop the columns we don't need \n",
    "df = df[['proposal_id', 'datetime', 'title', 'tags', \n",
    "         'borough', 'neighborhood', 'zipcode',\n",
    "         'audience', 'challenge', 'proposal']]\n",
    "\n",
    "# Standardize the audience column\n",
    "def standardize_audience(audience):\n",
    "    if pd.isna(audience):\n",
    "        return []\n",
    "    elif '[' not in audience:\n",
    "        return [audience]\n",
    "    else:\n",
    "        return eval(audience)\n",
    "\n",
    "df['audience'] = df['audience'].apply(standardize_audience)\n",
    "\n",
    "# Fill missing values with empty strings\n",
    "df.fillna('', inplace=True)\n",
    "\n",
    "# Ambiguous phrases that indicate that part of the proposal is not provided \n",
    "idk_phrases = [\n",
    "    'no', 'not sure', 'not given', 'unknown', \n",
    "    'none', 'not provided', 'not applicable', \n",
    "    'n/a', 'no idea', 'no response', 'no answer', \n",
    "    'no information', 'no data', 'no clue', \n",
    "    'no details', 'no comment', 'no opinion',\n",
    "    'nothing entered', 'nothing provided',\n",
    "    'nothing given', 'nothing specified',\n",
    "    '[illegible]','left blank', 'left empty',\n",
    "    'left unanswered', 'left unspecified',\n",
    "    \"i don't know\", \"non given\", \"non provided\",\n",
    "    \"non applicable\", \"see above?\", \"see above\",\n",
    "    \"nothing written\",\"--\",\"non\",\"none listed\",\n",
    "    \"nothing written here\",\"none given\",\"na\",\"to be determined\"\n",
    "]\n",
    "\n",
    "# Remove exact matches in title, challenge, and proposal columns \n",
    "for col in ['title', 'challenge', 'proposal']:\n",
    "    df[col] = df[col].apply(lambda x: '' if x.lower() in idk_phrases else x)\n",
    "\n",
    "# Concatenate the title, challenge, and proposal columns into a single column\n",
    "df['proposal'] = (df.title.str.strip() + '\\n' \n",
    "                  + df.challenge.str.strip() + '\\n' \n",
    "                  + df.proposal.str.strip()\n",
    "                  ).str.strip()\n",
    "\n",
    "# Drop the challenge column\n",
    "df.drop(columns=['challenge'], inplace=True)\n",
    "\n",
    "# Remove rows with less than 5 words in the proposal column \n",
    "df = df[df['proposal'].str.split().apply(len)>4]\n",
    "\n",
    "# Export the data\n",
    "df.to_excel('../data/nyc_pb_proposals_cleaned.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the number of words in the proposals\n",
    "fig = px.histogram(df, x=df['proposal'].str.split().str.len(), nbins=100)\n",
    "fig.update_traces(marker=dict(line=dict(width=1, color='DarkSlateGrey')))\n",
    "fig.update_layout(title='Number of Words in the Proposals')\n",
    "fig.update_xaxes(title='Number of Words')\n",
    "fig.update_yaxes(title='Number of Proposals')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings from OpenAI's `text-embedding-3-large` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the NYC Participatory Budgeting data\n",
    "data = pd.read_excel('../data/nyc_pb_proposals_cleaned.xlsx')\n",
    "\n",
    "# Initialize OpenAI client and embedding model\n",
    "client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "model = 'text-embedding-3-large'\n",
    "\n",
    "#Get the embeddings for the submission text (commented out since they are already stored in the data)\n",
    "data['submission_text_embedding_3_large'] = data.proposal.apply(\n",
    "   lambda text: get_embedding(\n",
    "      text=text, \n",
    "      client=client, \n",
    "      model=model,\n",
    "    )\n",
    ")\n",
    "data.to_json('../data/nyc_pb_proposals_text-embedding-3-large.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Prepare data for BERTopic\n",
    "# Read in NYC PB data with embeddings\n",
    "data = pd.read_json('../data/nyc_pb_proposals_text-embedding-3-large.json', orient='records', lines=True)\n",
    "\n",
    "# Get documents and pregenerated embeddings\n",
    "documents = data['proposal'].to_list()\n",
    "embeddings = np.array(data['submission_text_embedding_3_large'].to_list())\n",
    "\n",
    "print(f\"Number of documents: {len(documents)}\")\n",
    "print(f\"Sized of embeddings: {embeddings.shape}\")\n",
    "\n",
    "# 1. Embedding model\n",
    "# Initialize OpenAI client and embedding model\n",
    "client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "model = 'text-embedding-3-large'\n",
    "\n",
    "# Initialize the OpenAI backend for BERTopic\n",
    "embedding_model = OpenAIBackend(client, model)\n",
    "\n",
    "# 2. Dimensionality reduction \n",
    "#dim_model = PCA(n_components=31, random_state=seed)\n",
    "dim_model = UMAP(n_neighbors=15, n_components=28, min_dist=0.0, metric='cosine', random_state=seed)\n",
    "\n",
    "# 3. Clustering model\n",
    "# cluster_model = KMeans(\n",
    "#     n_clusters=15, \n",
    "#     init='k-means++', \n",
    "#     random_state=seed, \n",
    "#     max_iter=1000\n",
    "# )\n",
    "cluster_model = HDBSCAN(min_cluster_size=12, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "\n",
    "# 4. Vectorizer model\n",
    "# Get stop words to remove from the documents\n",
    "stop_words = get_stopwords()\n",
    "\n",
    "# Initialize the CountVectorizer model with the custom stop words and preprocessor\n",
    "vectorizer_model = CountVectorizer(\n",
    "    stop_words=stop_words, \n",
    "    preprocessor=preprocess_document, \n",
    "    min_df=2, \n",
    "    ngram_range=(1, 3)\n",
    ")\n",
    "\n",
    "# 5. Weighting model\n",
    "weighting_model = ClassTfidfTransformer(\n",
    "    reduce_frequent_words=True, \n",
    "    bm25_weighting=False,\n",
    "    # seed_words=seed_words,\n",
    "    # seed_multiplier=2\n",
    ")\n",
    "\n",
    "# 6. Representation models\n",
    "# Key BERT inspired model for first pass refinement of topics\n",
    "keybert_model = KeyBERTInspired(top_n_words=15)\n",
    "\n",
    "# Maximal Marginal Relevance model for diversity in topics\n",
    "mmr_model = MaximalMarginalRelevance(diversity=.3, top_n_words=15)\n",
    "\n",
    "# OpenAI model for final refinement of topics\n",
    "openai_model = openai_representation(\n",
    "    openai_api_key=os.getenv('OPENAI_API_KEY'), \n",
    "    model='gpt-4o-mini', \n",
    "    num_docs=10,\n",
    "    diversity=.3\n",
    ")\n",
    "\n",
    "# Chain the representation models together\n",
    "representation_models = {'KeyBERT':keybert_model,\n",
    "                         'KeyBERT+MMR':[keybert_model, mmr_model],\n",
    "                         'KeyBERT+MMR+OpenAI':[keybert_model, mmr_model, openai_model]}\n",
    "\n",
    "# Initialize BERTopic model with the pipeline models\n",
    "topic_model = BERTopic(\n",
    "    # Pipeline models\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=dim_model,\n",
    "    hdbscan_model=cluster_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    ctfidf_model=weighting_model,\n",
    "    # representation_model=representation_model,\n",
    "\n",
    "    # Hyperparameters\n",
    "    top_n_words=15,\n",
    "    verbose=True,\n",
    "    calculate_probabilities=True,\n",
    "    # nr_topics='auto',\n",
    "    # seed_topic_list=seed_topic_list\n",
    ")\n",
    "\n",
    "# Fit the BERTopic model to the data and get the topics and probabilities\n",
    "topics, probs = topic_model.fit_transform(\n",
    "    documents,      # List of documents\n",
    "    embeddings # Array of embeddings\n",
    ")\n",
    "\n",
    "# Reduce outliers based on probabilities of topics\n",
    "new_topics = topic_model.reduce_outliers(\n",
    "    documents, \n",
    "    topics, \n",
    "    probabilities=probs, \n",
    "    strategy=\"probabilities\",\n",
    "    threshold=0.04\n",
    ")\n",
    "\n",
    "# Update the topics based on the new topics\n",
    "topic_model.update_topics(\n",
    "    docs=documents, \n",
    "    topics=new_topics,\n",
    "    top_n_words=15,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    ctfidf_model=weighting_model,\n",
    "    representation_model=representation_models\n",
    ")\n",
    "\n",
    "# Evaluate the BERTopic model\n",
    "metrics = evaluate_topic_model(topic_model, documents, new_topics)\n",
    "\n",
    "# Create a dictionary of topic labels \n",
    "chatgpt_topic_labels = {topic: \" | \".join(list(zip(*values))[0]) for topic, values in topic_model.topic_aspects_[\"KeyBERT+MMR+OpenAI\"].items()}\n",
    "\n",
    "# Add outlier topic label\n",
    "chatgpt_topic_labels[-1] = \"Outlier Topic\" \n",
    "\n",
    "topic_model.set_topic_labels(chatgpt_topic_labels)\n",
    "\n",
    "# Get the topics and probabilities\n",
    "data['topics'] = topic_model.topics_\n",
    "data['probabilities'] = [max(prob) for prob in topic_model.probabilities_]\n",
    "\n",
    "# Merge data w/ topics \n",
    "topic_data = pd.merge(\n",
    "    data,\n",
    "    topic_model.get_topic_info(), \n",
    "    left_on='topics', \n",
    "    right_on='Topic'\n",
    ")\n",
    "\n",
    "# Filter columns\n",
    "topic_data = topic_data[['proposal_id', 'datetime', 'title', 'tags', 'borough', 'neighborhood',\n",
    "                         'zipcode', 'audience', 'proposal', 'submission_text_embedding_3_large',\n",
    "                         'topics', 'probabilities', 'Count', 'CustomName','KeyBERT+MMR']]\n",
    "# Rename columns\n",
    "topic_data.rename({'topics':'topic_id',\n",
    "                   'Count':'proposals_per_this_topic',\n",
    "                   'CustomName':'openai_topic_name',\n",
    "                   'KeyBERT+MMR':'keywords'}, axis=1, inplace=True)\n",
    "\n",
    "# Export the data\n",
    "# topic_data.to_excel('../data/nyc_pb_proposals_topics_20240806.xlsx', index=False)\n",
    "\n",
    "## Code below is for calculating cosine similarities to cluster centroids when using KMeans clustering\n",
    "# reduced_embeddings = dim_model.fit_transform(embeddings)\n",
    "# cluster_labels = cluster_model.fit_predict(reduced_embeddings)\n",
    "# centroids = cluster_model.cluster_centers_\n",
    "\n",
    "# # Calculate cosine similarities for each point to its cluster centroid\n",
    "# cosine_similarities_to_centroid = []\n",
    "# for i, embedding in enumerate(reduced_embeddings):\n",
    "#     centroid = centroids[cluster_labels[i]]\n",
    "#     # Reshape inputs for cosine_similarity function\n",
    "#     similarity = cosine_similarity(\n",
    "#         embedding.reshape(1, -1), \n",
    "#         centroid.reshape(1, -1)\n",
    "#     )\n",
    "#     cosine_similarities_to_centroid.append(similarity[0][0])\n",
    "\n",
    "# Add to DataFrame\n",
    "# topic_data['cosine_similarity_to_centroid'] = cosine_similarities_to_centroid\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations for Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram of probabilities\n",
    "fig = px.histogram(data[data['topics']==-1], x='probabilities', nbins=100)\n",
    "fig.update_traces(marker=dict(line=dict(width=1, color='DarkSlateGrey')))\n",
    "fig.update_layout(title='Probabilities of Topics')\n",
    "fig.update_xaxes(title='Probability')\n",
    "fig.update_yaxes(title='Number of Proposals')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cumulative distribution of probabilities \n",
    "fig = px.histogram(data[data['topics']==-1], x='probabilities', cumulative=True, histnorm='probability',nbins=100)\n",
    "fig.update_traces(marker=dict(line=dict(width=1, color='DarkSlateGrey')))\n",
    "fig.update_layout(title='Cumulative Distribution of Probabilities')\n",
    "fig.update_xaxes(title='Probability')\n",
    "fig.update_yaxes(title='Cumulative Probability')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of proposal topic probabilities by topic id\n",
    "data.groupby('topics')['probabilities'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic information\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics and their keyword scores\n",
    "fig = topic_model.visualize_barchart(\n",
    "    top_n_topics=39,\n",
    "    height=400, width=700,\n",
    "    n_words=15,\n",
    "    custom_labels=True\n",
    ")\n",
    "# increase x tick font size\n",
    "fig.update_yaxes(tickfont=dict(size=13))\n",
    "fig.write_html('../viz/word_score.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the number of proposals per topic\n",
    "agg_topic_data = topic_model.get_topic_info()\n",
    "agg_topic_data = agg_topic_data[agg_topic_data['Topic'] != -1]\n",
    "\n",
    "fig = px.bar(\n",
    "    agg_topic_data.sort_values(by='Count',ascending=False), \n",
    "    y='CustomName', \n",
    "    x='Count', \n",
    "    title='Proposals by Topic', \n",
    "    color='CustomName', \n",
    "    labels={'CustomName': '', 'Count': 'Number of Proposals'},\n",
    "    width=1200,\n",
    "    height=800,\n",
    "    orientation='h'\n",
    ")\n",
    "fig.update_layout(showlegend=False,plot_bgcolor='rgba(0, 0, 0, 0)')\n",
    "fig.update_xaxes(showgrid=True,gridcolor='lightgray')\n",
    "fig.write_html('../viz/proposal_distribution.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topic model similarity heatmap\n",
    "fig = topic_model.visualize_heatmap(custom_labels=True, width=1600, height=1050)\n",
    "fig.update_layout(title_text=\"\")\n",
    "fig.update_xaxes(tickfont=dict(size=14))\n",
    "fig.update_yaxes(tickfont=dict(size=14))\n",
    "fig.write_html('../viz/topic_similarity.html') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Similar Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the number of topics to 21\n",
    "new_topics = topic_model.reduce_topics(documents,nr_topics=21)\n",
    "\n",
    "# Set custom topic names\n",
    "chatgpt_topic_labels = {\n",
    "    topic: \" | \".join(list(zip(*values))[0]) \n",
    "    for topic, values in topic_model.topic_aspects_[\"KeyBERT+MMR+OpenAI\"].items()\n",
    "}\n",
    "# Add outlier topic label\n",
    "chatgpt_topic_labels[-1] = \"Outlier Topic\" \n",
    "topic_model.set_topic_labels(chatgpt_topic_labels)\n",
    "\n",
    "# Add topic ids and probabilities to the data\n",
    "data['topics'] = topic_model.topics_\n",
    "data['probabilities'] = [max(prob) for prob in topic_model.probabilities_]\n",
    "\n",
    "# Merge data w/ topics \n",
    "topic_data = pd.merge(\n",
    "    data,\n",
    "    topic_model.get_topic_info(), \n",
    "    left_on='topics', \n",
    "    right_on='Topic'\n",
    ")\n",
    "\n",
    "# Filter columns\n",
    "topic_data = topic_data[['proposal_id', 'datetime', 'title', 'tags', 'borough', 'neighborhood',\n",
    "                         'zipcode', 'audience', 'proposal', 'submission_text_embedding_3_large',\n",
    "                         'topics', 'probabilities', 'Count', 'CustomName','KeyBERT+MMR']]\n",
    "\n",
    "# Rename columns\n",
    "topic_data.rename({'topics':'topic_id',\n",
    "                   'Count':'proposals_per_this_topic',\n",
    "                   'CustomName':'openai_topic_name',\n",
    "                   'KeyBERT+MMR':'keywords'}, axis=1, inplace=True)\n",
    "\n",
    "# Export the data\n",
    "topic_data.to_excel('../data/nyc_pb_proposals_topics_20240806.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the number of proposals by topic and borough\n",
    "# Count of proposals by topic and borough, excluding outliers\n",
    "temp = topic_data[topic_data['topic_id'] != -1].groupby(['openai_topic_name', 'borough']).size().reset_index(name='proposals')\n",
    "fig = px.bar(temp, \n",
    "             y='openai_topic_name', \n",
    "             x='proposals', \n",
    "             color='borough',\n",
    "             title='Proposals by Topic and Borough',\n",
    "             labels={'openai_topic_name': 'Topic Name', 'proposals': 'Count of Proposals'},\n",
    "             width=1400,\n",
    "             height=1200)\n",
    "fig.update_layout(yaxis_title='Topic Name',\n",
    "                  xaxis_title='Number of Proposals',\n",
    "                  title={'text': 'Proposals by Topic and Borough', 'x':0.5},\n",
    "                  legend_title='Borough',\n",
    "                  barmode='group')\n",
    "fig.update_yaxes(tickfont=dict(size=15))\n",
    "fig.write_html('../viz/proposal_frequency_by_borough.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
